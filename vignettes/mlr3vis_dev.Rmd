---
title: "mlr3 vis development"
author: "Linlin Yin"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  collapse = TRUE,
  comment = "#>",
  fig.width=7,
  fig.height=7
)
```

# Test for visulization functions from mlr to mlr3

```{r}
#library(mlr3)
#library(ggplot2)
#library(data.table)
#library(dplyr)
#library(checkmate)

#devtools::load_all("d:/source/mlr3vis/")
library(mlr3)
library(mlr3vis)

```

mlr: https://mlr.mlr-org.com/articles/tutorial/predict.html  

mlr3: https://mlr3book.mlr-org.com/  

## plotLearnerPrediction

```{r}

task = mlr_tasks$get("iris")
learner = mlr_learners$get("classif.rpart")

plotLearnerPrediction(learner=learner,task=task,prob.alpha = FALSE)


plotLearnerPrediction(learner=learner,task=task,features=c("Sepal.Length","Sepal.Width"))

```


## plotLearnerPrediction based on Experiment

```{r}

task = mlr_tasks$get("iris")
learner = mlr_learners$get("classif.rpart")
e = Experiment$new(task = task, learner = learner)$train()$predict()
e$performance #NULL
e$score()$performance

plotLearnerPredictionExperiment(e)

```

## summaryFeatureByPrediction based on Experiment
```{r,results='asis'}

summaryFeatureByPrediction(e,groupVar="truth")

summaryFeatureByPrediction(e,groupVar="response")

```


## plotFeatureByPrediction based on Experiment
```{r}

plotFeatureByPrediction(e,groupVar="truth")

plotFeatureByPrediction(e,groupVar="response",style = "dot")

```




## benchmark related
```{r}

# get some example tasks
tasks = mlr_tasks$mget(c("pima", "sonar", "spam"))

# set measures for all tasks: accuracy (acc) and area under the curve (auc)
measures = mlr_measures$mget(c("classif.acc", "classif.auc"))
tasks = lapply(tasks, function(task) { task$measures = measures; task })

# get a featureless learner and a classification tree
learners = mlr_learners$mget(c("classif.featureless", "classif.rpart"))

# let the learners predict probabilities instead of class labels (required for AUC measure)
learners$classif.featureless$predict_type = "prob"
learners$classif.rpart$predict_type = "prob"

# compare via 10-fold cross validation
resamplings = mlr_resamplings$mget("cv")

# create a BenchmarkResult object
design = expand_grid(tasks, learners, resamplings)
print(design)


bmr = benchmark(design)

#bmr
#bmr$aggregated(objects = FALSE)
#bmr$aggregated(objects = FALSE)[, list(acc = mean(classif.acc), auc = mean(classif.auc)), by = "learner_id"]





```

### plotBMRBoxplots
```{r}

plotBMRBoxplots(bmr)
#Different facet and violin plot
plotBMRBoxplots(bmr,style = "violin",xVar="task_id",facet_x="measure",facet_y="learner_id")
#Dot plot
plotBMRBoxplots(bmr,style = "dot")

```


### plotBMRRanksAsBarChart
```{r}
plotBMRRanksAsBarChart(bmr)
```

## generateThreshVsPerfData and plotThreshVsPerf

```{r}

task = mlr_tasks$get("sonar")
task$measures
measures = mlr_measures$mget(c("classif.acc","classif.ce","classif.fpr","classif.tpr"))
task$measures = measures

learner = mlr_learners$get("classif.rpart",predict_type = "prob")
#learner$predict_type
#learner$predict_type = "prob"
e = Experiment$new(task = task, learner = learner)$train()$predict()
e$performance #NULL
e$score()$performance

performanceByThreshold=generateThreshVsPerfData(e)
plotThreshVsPerf(performanceByThreshold)

```

## ROC based on generateThreshVsPerfData
```{r}

plotROCCurves(performanceByThreshold)

```

